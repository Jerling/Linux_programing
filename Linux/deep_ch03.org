#+TITLE: 内存管理

* Table of Contens  :TOC_2_gh:
- [[#概述][概述]]
  - [[#n-uma-模型中的内存组织][(N) UMA 模型中的内存组织]]

* 概述
    内存管理的实现涵盖许多领域：

    - 物理内存页的管理
    - 分配大块内存的伙伴系统
    - 分配小块的 slab、slub和slob分配器
    - 分配非连续的 =vmalloc= 机制
    - 进程的地址空间 

    管理内存的方法：

    1. UMA 计算机： 一致性访问，将可用内存以连续的方式组织起来
    2. NUMA ： 非一致性访问，总是多处理计算机，各个 CPU 都有本地内存，支持特别快速的访问。 CPU 之间通过总线连接起来，便于其他 CPU 的访问

    [[file:img/Snipaste_2019-01-15_17-16-36.png]]

** (N) UMA 模型中的内存组织
*** 概述
    先考虑 NUMA 系统：

    1. 首先划分内存为结点，每个结点关联到系统中的一个处理器
    2. 各个结点又划分为内存域，是内存的进一步细分
#+BEGIN_SRC c
enum zone_type {
#ifdef CONFIG_ZONE_DMA
	/*
	 * ZONE_DMA is used when there are devices that are not able
	 * to do DMA to all of addressable memory (ZONE_NORMAL). Then we
	 * carve out the portion of memory that is needed for these devices.
	 * The range is arch specific.
	 *
	 * Some examples
	 *
	 * Architecture		Limit
	 * ---------------------------
	 * parisc, ia64, sparc	<4G
	 * s390			<2G
	 * arm			Various
	 * alpha		Unlimited or 0-16MB.
	 *
	 * i386, x86_64 and multiple other arches
	 * 			<16M.
	 */
	ZONE_DMA,  // 标记合适的DMA的内存区域
#endif
#ifdef CONFIG_ZONE_DMA32
	/*
	 * x86_64 needs two ZONE_DMAs because it supports devices that are
	 * only able to do DMA to the lower 16M but also 32 bit devices that
	 * can only do DMA areas below 4G.
	 */
	ZONE_DMA32,  // 标记了使用 32 位地址可寻址，适合 DMA 的内存域
#endif
	/*
	 * Normal addressable memory is in ZONE_NORMAL. DMA operations can be
	 * performed on pages in ZONE_NORMAL if the DMA devices support
	 * transfers to all addressable memory.
	 */
	ZONE_NORMAL,  // 标记可直接映射到内核段的普通内存域
#ifdef CONFIG_HIGHMEM
	/*
	 * A memory area that is only addressable by the kernel through
	 * mapping portions into its own address space. This is for example
	 * used by i386 to allow the kernel to address the memory beyond
	 * 900MB. The kernel will set up special mappings (page
	 * table entries on i386) for each page that the kernel needs to
	 * access.
	 */
	ZONE_HIGHMEM,  // 标记超出内核段的物理内存
#endif
	ZONE_MOVABLE,
#ifdef CONFIG_ZONE_DEVICE ZONE_DEVICE,
#endif
	__MAX_NR_ZONES

};
#+END_SRC
**** 结点管理
     pg_data_t 是用于表示结点的基本元素：

#+BEGIN_SRC C
// include/linux/mmzone.h
typedef struct pglist_data {
	struct zone node_zones[MAX_NR_ZONES];
	struct zonelist node_zonelists[MAX_ZONELISTS];
	int nr_zones;
#ifdef CONFIG_FLAT_NODE_MEM_MAP	/* means !SPARSEMEM */
	struct page *node_mem_map;
#ifdef CONFIG_PAGE_EXTENSION
	struct page_ext *node_page_ext;
#endif
#endif
#if defined(CONFIG_MEMORY_HOTPLUG) || defined(CONFIG_DEFERRED_STRUCT_PAGE_INIT)
	/*
	 * Must be held any time you expect node_start_pfn, node_present_pages
	 * or node_spanned_pages stay constant.  Holding this will also
	 * guarantee that any pfn_valid() stays that way.
	 *
	 * pgdat_resize_lock() and pgdat_resize_unlock() are provided to
	 * manipulate node_size_lock without checking for CONFIG_MEMORY_HOTPLUG
	 * or CONFIG_DEFERRED_STRUCT_PAGE_INIT.
	 *
	 * Nests above zone->lock and zone->span_seqlock
	 */
	spinlock_t node_size_lock;
#endif
	unsigned long node_start_pfn;
	unsigned long node_present_pages; /* total number of physical pages */
	unsigned long node_spanned_pages; /* total size of physical page
					     range, including holes */
	int node_id;
	wait_queue_head_t kswapd_wait;
	wait_queue_head_t pfmemalloc_wait;
	struct task_struct *kswapd;	/* Protected by
					   mem_hotplug_begin/end() */
	int kswapd_order;
	enum zone_type kswapd_classzone_idx;

	int kswapd_failures;		/* Number of 'reclaimed == 0' runs */

#ifdef CONFIG_COMPACTION
	int kcompactd_max_order;
	enum zone_type kcompactd_classzone_idx;
	wait_queue_head_t kcompactd_wait;
	struct task_struct *kcompactd;
#endif
	/*
	 * This is a per-node reserve of pages that are not available
	 * to userspace allocations.
	 */
	unsigned long		totalreserve_pages;

#ifdef CONFIG_NUMA
	/*
	 * zone reclaim becomes active if more unmapped pages exist.
	 */
	unsigned long		min_unmapped_pages;
	unsigned long		min_slab_pages;
#endif /* CONFIG_NUMA */

	/* Write-intensive fields used by page reclaim */
	ZONE_PADDING(_pad1_)
	spinlock_t		lru_lock;

#ifdef CONFIG_DEFERRED_STRUCT_PAGE_INIT
	/*
	 * If memory initialisation on large machines is deferred then this
	 * is the first PFN that needs to be initialised.
	 */
	unsigned long first_deferred_pfn;
	/* Number of non-deferred pages */
	unsigned long static_init_pgcnt;
#endif /* CONFIG_DEFERRED_STRUCT_PAGE_INIT */

#ifdef CONFIG_TRANSPARENT_HUGEPAGE
	spinlock_t split_queue_lock;
	struct list_head split_queue;
	unsigned long split_queue_len;
#endif

	/* Fields commonly accessed by the page reclaim scanner */
	struct lruvec		lruvec;

	unsigned long		flags;

	ZONE_PADDING(_pad2_)

	/* Per-node vmstats */
	struct per_cpu_nodestat __percpu *per_cpu_nodestats;
	atomic_long_t		vm_stat[NR_VM_NODE_STAT_ITEMS];
} pg_data_t;
#+END_SRC

- node_zones : 是一个数组，包含了结点中各内存域的数据结构
- node_zonelists : 指定了备用结点及其内存域的列表
- 结点中不同内存域的数目保存在 nr_zones
- node_mem_map 是指向 page 实例数组的指针，用来描述结点所有物理内存页
- 在系统启动期间，内存初始化内存管理子系统之前，内核也需要使用内存
- node_start_pfn : 该 NUMA 结点第一个页帧的逻辑编号
- node_id : 是全局结点 ID
- pg_data_next : 连接到下一个内存结点
- kswapd_wait : 交换守护进程等待队列，在将页帧换出结点时会用到
***** 结点状态管理
如果系统中的节点多余一个，内核会维护一个位图来提供各个结点的状态信息，用位掩码指
定。

#+BEGIN_SRC C
// https://elixir.bootlin.com/linux/v5.0-rc1/source/include/linux/nodemask.h#L391
/*
 * Bitmasks that are kept for all the nodes.
 */
enum node_states {
	N_POSSIBLE,		/* The node could become online at some point */
	N_ONLINE,		/* The node is online */
	N_NORMAL_MEMORY,	/* The node has regular memory */
#ifdef CONFIG_HIGHMEM
	N_HIGH_MEMORY,		/* The node has regular or high memory */
#else
	N_HIGH_MEMORY = N_NORMAL_MEMORY,
#endif
	N_MEMORY,		/* The node has memory(regular, high, movable) */
	N_CPU,		/* The node has one or more cpus */
	NR_NODE_STATES
};
#+END_SRC

- N_POSSIBLE, N_ONLINE, N_CPU 用于 CPU 和内存的热拔插
- 如果有普通或高端内存，则使用 N_HIGH_MEMORY, 否则用 N_NORMAL_MEMORY
  
对应的辅助函数：
  
#+BEGIN_SRC C
static inline void node_set_state(int node, enum node_states state)
{
	__node_set(node, &node_states[state]);
}

static inline void node_clear_state(int node, enum node_states state)
{
	__node_clear(node, &node_states[state]);
}

for_each_node_state(__node, __state) // 迭代处于特定的状态的所有结点
#+END_SRC
***** 内存域
内核使用 zone 结构描述内存域：

#+BEGIN_SRC C
// https://elixir.bootlin.com/linux/v5.0-rc1/source/include/linux/mmzone.h#L362
struct zone {
	/* Read-mostly fields */

	/* zone watermarks, access with *_wmark_pages(zone) macros */
	unsigned long _watermark[NR_WMARK];
	unsigned long watermark_boost;

	unsigned long nr_reserved_highatomic;

	/*
	 * We don't know if the memory that we're going to allocate will be
	 * freeable or/and it will be released eventually, so to avoid totally
	 * wasting several GB of ram we must reserve some of the lower zone
	 * memory (otherwise we risk to run OOM on the lower zones despite
	 * there being tons of freeable ram on the higher zones).  This array is
	 * recalculated at runtime if the sysctl_lowmem_reserve_ratio sysctl
	 * changes.
	 */
	long lowmem_reserve[MAX_NR_ZONES];

#ifdef CONFIG_NUMA
	int node;
#endif
	struct pglist_data	*zone_pgdat;
	struct per_cpu_pageset __percpu *pageset;

#ifndef CONFIG_SPARSEMEM
	/*
	 * Flags for a pageblock_nr_pages block. See pageblock-flags.h.
	 * In SPARSEMEM, this map is stored in struct mem_section
	 */
	unsigned long		*pageblock_flags;
#endif /* CONFIG_SPARSEMEM */

	/* zone_start_pfn == zone_start_paddr >> PAGE_SHIFT */
	unsigned long		zone_start_pfn;

	/*
	 * spanned_pages is the total pages spanned by the zone, including
	 * holes, which is calculated as:
	 * 	spanned_pages = zone_end_pfn - zone_start_pfn;
	 *
	 * present_pages is physical pages existing within the zone, which
	 * is calculated as:
	 *	present_pages = spanned_pages - absent_pages(pages in holes);
	 *
	 * managed_pages is present pages managed by the buddy system, which
	 * is calculated as (reserved_pages includes pages allocated by the
	 * bootmem allocator):
	 *	managed_pages = present_pages - reserved_pages;
	 *
	 * So present_pages may be used by memory hotplug or memory power
	 * management logic to figure out unmanaged pages by checking
	 * (present_pages - managed_pages). And managed_pages should be used
	 * by page allocator and vm scanner to calculate all kinds of watermarks
	 * and thresholds.
	 *
	 * Locking rules:
	 *
	 * zone_start_pfn and spanned_pages are protected by span_seqlock.
	 * It is a seqlock because it has to be read outside of zone->lock,
	 * and it is done in the main allocator path.  But, it is written
	 * quite infrequently.
	 *
	 * The span_seq lock is declared along with zone->lock because it is
	 * frequently read in proximity to zone->lock.  It's good to
	 * give them a chance of being in the same cacheline.
	 *
	 * Write access to present_pages at runtime should be protected by
	 * mem_hotplug_begin/end(). Any reader who can't tolerant drift of
	 * present_pages should get_online_mems() to get a stable value.
	 */
	atomic_long_t		managed_pages;
	unsigned long		spanned_pages;
	unsigned long		present_pages;

	const char		*name;

#ifdef CONFIG_MEMORY_ISOLATION
	/*
	 * Number of isolated pageblock. It is used to solve incorrect
	 * freepage counting problem due to racy retrieving migratetype
	 * of pageblock. Protected by zone->lock.
	 */
	unsigned long		nr_isolate_pageblock;
#endif

#ifdef CONFIG_MEMORY_HOTPLUG
	/* see spanned/present_pages for more description */
	seqlock_t		span_seqlock;
#endif

	int initialized;

	/* Write-intensive fields used from the page allocator */
	ZONE_PADDING(_pad1_)

	/* free areas of different sizes */
	struct free_area	free_area[MAX_ORDER];

	/* zone flags, see below */
	unsigned long		flags;

	/* Primarily protects free_area */
	spinlock_t		lock;

	/* Write-intensive fields used by compaction and vmstats. */
	ZONE_PADDING(_pad2_)

	/*
	 * When free pages are below this point, additional steps are taken
	 * when reading the number of free pages to avoid per-cpu counter
	 * drift allowing watermarks to be breached
	 */
	unsigned long percpu_drift_mark;

#if defined CONFIG_COMPACTION || defined CONFIG_CMA
	/* pfn where compaction free scanner should start */
	unsigned long		compact_cached_free_pfn;
	/* pfn where async and sync compaction migration scanner should start */
	unsigned long		compact_cached_migrate_pfn[2];
#endif

#ifdef CONFIG_COMPACTION
	/*
	 * On compaction failure, 1<<compact_defer_shift compactions
	 * are skipped before trying again. The number attempted since
	 * last failure is tracked with compact_considered.
	 */
	unsigned int		compact_considered;
	unsigned int		compact_defer_shift;
	int			compact_order_failed;
#endif

#if defined CONFIG_COMPACTION || defined CONFIG_CMA
	/* Set to true when the PG_migrate_skip bits should be cleared */
	bool			compact_blockskip_flush;
#endif

	bool			contiguous;

	ZONE_PADDING(_pad3_)
	/* Zone statistics */
	atomic_long_t		vm_stat[NR_VM_ZONE_STAT_ITEMS];
	atomic_long_t		vm_numa_stat[NR_VM_NUMA_STAT_ITEMS];
} ____cacheline_internodealigned_in_smp; // 编译器关键字，实现最优的高速缓存对齐方式
#+END_SRC

由于对 zone 结构访问比较频繁，因此应 ZONE_PADDING 分隔为几个部分。多处理器系统有
不同的 CPU 试图同时访问结构成员，使用锁防止干扰。

- 如果空闲页多于 pages_high, 则内存域的状态是理想的
- 如果空闲页的数目低于 pages_low, 则内核开始将页换出到硬盘
- 如果空闲的数目低于 page_min, 页回收工作压力比较大，内村域急需空闲页
- lovmem_reserve数组分别为各种内存域指定了若干页，用于一些无论如何都不能失败的关
  键性内存分配。
- pageset是一个数组，用于实现每个CPU的热/冷页帧列表。
- free_area 是同名数据结构的数组，用于实现伙伴系统
***** 内存水位的计算
      在计算之前，内核首先为关键性分配保留内存空间的最小值，该值随可用内存的大小而非线性证章，保存在全局变量 min_free_kbytes 中，
有一个约束是该值只能在 [128k~64M] 之间。

内核在启动期间调用 init_per_zone_min 填充水印值，无须显示调用。 setup_per_zone_pages_min 设置 struct zone 的 pages_min, pages_low, pages_high 成员。
***** 冷热页
      pageset 实现冷热分配器，热页说明已经加载到 CPU 高速缓存。
      
pageset 是一个数组，其容量与系统能容纳的CPU数目的最大值相同
***** 页帧
      系统中内存的最小单位，对每个页都会创建一个 struct page 实例。 page 源码：https://elixir.bootlin.com/linux/v5.0-rc1/source/include/linux/mm_types.h#L70
      slab, freelist, inuse 成员用于 slub 分配器。

- flags 存储了体系结构无关的标志，用于描述页的属性。
- _count 是一个使用计数，表示内核中该页的次数
- _mapcount表示在页表中有多少项指向该页
- lru用于在各种链表上维护该页
- 内核可以将多个毗邻的页合为较大的复合页
- mapping 指定了页帧所在的地址空间
- private是一个指向“私有”数据的指针，虚拟内存管理会忽略该数据。
- virtual用于高端内存区域中的页，换言之，即无法汽接映射到内核内存中的页。
****** 体系结构无关的页标志
- pagelocked : 查询比特位是否置位
- SetPageLocked : 设置户 pG_locked 位，不考虑先前的状态，
- TestSetPageLocked : 设置比特位，而月.返回原值:
- ClearPageLocked : 清除比特位，不考虑先前的状态;
- TestClearPageLocked : 清除比特位，返回原值。
